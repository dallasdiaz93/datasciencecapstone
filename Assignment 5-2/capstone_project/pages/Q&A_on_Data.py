# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1568aDHUuTbLxdvdPai2XR0JFKMIJUXZJ
"""

import streamlit as st
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAI
from langchain.chains import RetrievalQA
from langchain_community.document_loaders.directory import DirectoryLoader
import os
import nltk


openai_api_key = os.getenv("OPENAI_API_KEY", "")

# Get your loader ready
loader = DirectoryLoader('content/', glob='**/*.txt')

# Load up your text into documents
documents = loader.load()

# Get your text splitter ready
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)

# Split your documents into texts
texts = text_splitter.split_documents(documents)

# Turn your texts into embeddings
embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)

# Get your docsearch ready
docsearch = FAISS.from_documents(texts, embeddings)

# Load up your LLM
llm = OpenAI(openai_api_key=openai_api_key)

# Create your Retriever
qa = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=docsearch.as_retriever())

# Run a query
query = "give me list of 5 common themes from the layoff story data?"
qa.invoke(query)

qa = RetrievalQA.from_chain_type(llm=llm,
                                chain_type="stuff",
                                retriever=docsearch.as_retriever(),
                                return_source_documents=True)
query = "give me list of 5 common themes from the layoff story data??"
#result = qa.invoke({"query": query})

#print(result['result'])

#result['source_documents']

# Define the Streamlit app layout
st.title("Let's Talk with Data!")
query = st.text_input("Enter your query:", " ")



# Button to submit the query
if st.button("Submit"):
    # Invoke the question-answering model with the user input
    result = qa.invoke({"query": query})
    
    # Display the results
    st.write("Result:")
    st.write(result['result'])
else:
    # Display placeholder for empty results
    st.write("Result: (waiting for query submission)")
# Invoke the question-answering model with the user input
#result = qa.invoke({"query": query})

# Display the results
#st.write("Result:")
#st.write(result['result'])